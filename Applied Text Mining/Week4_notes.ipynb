{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic similiraty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull when:\n",
    "- grouping similar words into semantic concepts into concepts that have the same meaning/appears to have the same meaning.\n",
    "- as a building block in natural language understanding tasks, like paraphrasing or textul eintailment(= smaller sentence derives its meaning or entails its meaning from another piece of text).\n",
    "    - ex. you have a sentence and a text passage and based on the info in the text passage you need to say whether the sentence derives its meaning from there or not \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- usefull resources is WOrdeNet --> semantic dictionary of words interlinked by semantic relationships.\n",
    "- all the words, type of words, verbs, noun etc are in Hierarchies.\n",
    "- you need a measure for defining semantic similarity and that is:\n",
    "\n",
    "    - Path similiraty: finding shortest path between these 2 concepts in this hierarchy\n",
    "    - LCS Lowest common subsumer: root of tree (ancestor) that is closest to both concepts, ex. giraffe-horse --> 4legged animal\n",
    "        - it is measured as follows called Lin Similarity:\n",
    "        - LinSim(u,v) = 2*log Prob(LCS(u,v)) / (log Prob(u) +log Prob(v))\n",
    "        - P(u) is given by the information content learned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T09:55:02.664153Z",
     "start_time": "2018-04-20T09:55:02.659217Z"
    }
   },
   "outputs": [],
   "source": [
    "# import WordNet\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:07:30.824885Z",
     "start_time": "2018-04-20T10:07:30.813748Z"
    }
   },
   "outputs": [],
   "source": [
    "# find appropriate similar words\n",
    "\n",
    "deer=wn.synset('deer.n.01') # 'find synset of deer in the sense given by a noun of it and the first meaning of it'\n",
    "elk= wn.synset('elk.n.01')\n",
    "horse=wn.synset('horse.n.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### path similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T09:57:42.614794Z",
     "start_time": "2018-04-20T09:57:42.602774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.path_similarity(elk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:07:41.193473Z",
     "start_time": "2018-04-20T10:07:41.182443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.path_similarity(horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using LinSim and information criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:09:44.423452Z",
     "start_time": "2018-04-20T10:09:43.781670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8623778273893673"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "brown_ic=wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "deer.lin_similarity(elk,brown_ic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:10:06.397063Z",
     "start_time": "2018-04-20T10:10:06.389039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7726998936065773"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.lin_similarity(horse, brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is not using the distance between 2 concepts explicitly\n",
    "--> deer and horse were very far awy in WN hierarchy but still get the higher higher significance and higher similiraty because in tipical contexts the information that is contained by these words deer and horse are enough close because theyre both animals/mammals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional similarity and Collocations\n",
    "\n",
    "- 2 words that frequently appear in similar contexts are more likely to be semantically relate\n",
    "- can give constraints like: \n",
    "    - define contex based on words before, after, or within small window of target word\n",
    "    - use also parts of speech as context\n",
    "    - this particular target word occurs right after a determiner or after location etc..\n",
    "    - specific syntactic relation to the target\n",
    "\n",
    "--> compute strenght of association between words after context is defined, based on how frequently these words co-worker or how frequnetly they collocate --> Collocations\n",
    "\n",
    "- ex: have 2 words that keep coming next to each other then youd say that hey are very highly related, \n",
    "- its also importanto to see how frequent individual words are\n",
    "    - 'the' is so freq that it co-ocurs often with every words \n",
    "    - need to normalize this frequncy:\n",
    "        - --> Pointwise Mutual Information\n",
    "        - PMI(w,c)= log[ Prob(w,c) / Prob(w)*Prob(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T11:11:35.788381Z",
     "start_time": "2018-04-20T11:11:35.760808Z"
    }
   },
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(text) #learn it from text\n",
    "finder.nbest(bigram_measures.pmi, 10) # get top10 pairs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T11:13:14.792026Z",
     "start_time": "2018-04-20T11:13:14.772973Z"
    }
   },
   "source": [
    "# restrict any pair that does not occur at least 10times in the corpus\n",
    "finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A coarse level of whats in a text collection --> make sense of the texts topic\n",
    "- topic is the subject of the discouse\n",
    "- topics are represented as word distribution\n",
    "- Document is a mixture of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whats known:\n",
    "- text collection or corpus\n",
    "- number of topics\n",
    "\n",
    "Not known:\n",
    "- the actual topics\n",
    "- topic distribution for each doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic modelling is basically a text clustering problem\n",
    "Typical model approaches are:\n",
    "- Probabilistic Latent Semantic Analysis PLSA\n",
    "- Latent Dirichlet Allocation LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- is a generative model ( from list of words it creates/generate a full document)\n",
    "- choose the lenght of doc d\n",
    "- choose mix of topics\n",
    "- use topics multinomial dsitribution to output words to fill that topics quota\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose that for a particular doc, 40% of words come from topic A, then use that topics A multi Distribution to output 40% of th words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many topics do i want?\n",
    "- hard to answer...\n",
    "\n",
    "interpreting topics\n",
    "- topics are just word distribution\n",
    "- making sense of topics is subjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- need to tokenize, normalize\n",
    "- stop word removal ( words that are frequnt but not meaningful in that domain, 'the' 'in' 'is' ) depends on document\n",
    "- stemming\n",
    "\n",
    "- convert tokenized docs to a doc-term matrix\n",
    "- build lda on top of temr matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "doc_set: set of pre-process text docs\n",
    "    \n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_set) #is mapping of words and IDs\n",
    "corpus = [dicitonary.doc2bow(doc) for doc in doc_set] # from doc to bag of words model// creates the doc-term matrix\n",
    "\n",
    "LDAmodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word= dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=4, num_words=5)) #print top5 words of the 4 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
